# MitM Attack Statistics-Simulator
Man-in-the-Middle Attack Statistics Simulator (using Apache Spark)

## Project Description

<div style="text-align: justify;">

The Man-in-the-Middle Attack Statistics Simulator is a distributed program using Apache Spark to simulate and compute statistics for a Man-in-the-Middle (MitM) attack. It efficiently processes large graphs that simulate a real-world, large company's internal network. For this project, large-scale graphs generated by the [NetGameSim](https://github.com/0x1DOCD00D/NetGameSim) platform are used as input. The primary objective of this project is to compute statistics on successful and unsuccessful MitM attacks based on random communications within the network.

</div>

## Requirements
- **Java Development Toolkit (JDK)**:
  - Version: Oracle OpenJDK 11.0.2
  
- **Simple Build Toolkit (SBT)**:
  - Installation: [MSI Installer](https://www.scala-sbt.org/1.x/docs/Installing-sbt-on-Windows.html)
  - Version in Project: 1.8.3 (specified in `project/build.properties`)
  
- **Development Environment**:
  - Scala Version: 2.12.15 (compatible with AWS EMR-6.13.0)
  - Spark Version: 3.4.1 (compatible with AWS EMR-6.13.0)
  - IDE: IntelliJ IDEA Ultimate
  
- **Other Dependencies**:
  - Managed in `build.sbt` file
  
> Ensure you can create, compile, and run both Java and Scala programs in your environment to proceed with the project.<br>


## How to install and run the project

1. Clone the project. The base directory should be "CS441- MitMStatSim"<br>

> [!IMPORTANT]
> - Read the following to learn about how to generate new input graph files- 
> This project utilizes case classes from [NetGameSim](https://github.com/0x1DOCD00D/NetGameSim) to define nodes and edges of graphs. NetGameSim generates ".ngs" and ".ngs.perturbed" files. To use these files with this project, you need to convert them to ".txt" and ".txt.perturbed" formats using additional code provided in this repository's path: `base directory/NetGraph/src/main/scala/NGStoText.scala`. Follow the instructions in the comments of `NGStoText.scala` to integrate this code into NetGameSim's `base directory/src/main/scala/` directory.

2. Navigate to the `application.conf` file and configure settings. This file contains all necessary folder and file path configurations for the project. All configurations are managed in `Utilities/src/main/resources/application.conf`.

Refer to the following block for `application.conf` parameters:

```md
- `NGSGraphDir`: Path to the directory containing all graph files (`someOriginalGraph.txt`, `somePerturbedGraph.txt.perturbed`).
- `originalGraphFileName`: File name of the original graph (must have `.txt` extension).
- `perturbedGraphFileName`: File name of the perturbed graph (must have `.txt.perturbed` extension).
- `statisticsOutputFileName`: File name for the final statistics output (must have `.yaml` extension).
- `masterURL`: Specifies the file system:
  - Set to `"hdfs://localhost:<port>"` for Hadoop Distributed File System.
  - Set to `"local"` for local file system.
  - Set to `"s3://<your_bucket_name>"` for Amazon S3 bucket.
  Choose carefully as it affects application behavior.
- `master`: Specifies the Spark master type. Can be set via `spark-submit` with `--master <master-type>`. Refer to [Spark master URLs](https://spark.apache.org/docs/latest/submitting-applications.html) for details.
- `randomWalkCoeff`: Fraction of nodes to traverse in one random walk on a graph.
- `numOfParallelWalks`: Number of partitions, each assigned a starting node.
- `numItersPerCompNode`: Total number of random walks to execute per partition.
- `itersBeforeAccum`: Number of iterations per partition before results are accumulated.
- `nodeMatchThreshold`: Threshold for SimRank algorithm; nodes with similarity scores below this threshold are rejected.

```

 > [!NOTE]
 > As you might've noticed in the application.conf, The same variables apply for local, HDFS, or AWS S3 environments. One command-line argument must be passed at runtime to specify your environment (detailed in point 4 below).

3. Create folders as specified in `application.conf`. Use the `input` and `output` folders provided in the root directory. Ensure all input graph files (converted from NetGameSim) are in the `input` folder, and the output statistics YAML file is saved in the `output` folder.

4. Save the two graph files with .txt and .txt.perturbed extensions in the `NGSGraphDir` folder.

5. Running Locally
   From the terminal, execute `sbt compile`. Then, run `sbt "run args(0)"`. This command requires one command-line argument:
   - `args(0)`: Set to "**local**", "**hdfs**", or "**aws**" to load environment variables from `application.conf`.

6. Running on Amazon EMR

    - **6.1** Set paths under the AWS scope in `application.conf`. Set `masterURL` to `s3://<your-bucket-name>`. Remember, all paths should conform to the Amazon S3 file system and start with `s3`.

    - **6.2** From the terminal, go to the `Main.scala` path and run `sbt clean compile assembly`. This should create a fat JAR file named `MitmStatSim.jar` in `basedirectory/target/scala-2.12/`.

    - **6.3** Create two directories in the AWS bucket - one for the input graph text files and another for the output YAML files.

    - **6.4** Upload the graph files to the `input` directory in the S3 bucket. Upload the JAR file to the S3 root directory. There is no specific need to create these exact folders; this is one way to configure it.

    - **6.5** Create a cluster in Amazon EMR. Use the Spark application specified in the step above. Provide the custom JAR S3 file path URL. In the `spark-submit` section, use the following parameters:
   ```plaintext
   --class mitm.Main
   --jars /your/s3/jar/file/path
   --driver-class-path /your/s3/jar/file/path
    ```
   - **6.7** Set the command line argument to <ins>`aws`</ins> before running the configured EMR cluster.<br>

## Understanding Important Code Files

<table><tr><td>1.<ins>Main.scala</ins></td></tr></table>
<div style="text-align: justify;">
This file contains the `main()` function. It orchestrates the process by reading the graph files, setting up Spark configurations, and initializing necessary operations. Key functionalities include identifying starting nodes without incoming edges, partitioning the workload using `numOfParallelWalks`, broadcasting graphs across computing units, and performing parallel random walks. Optimization is achieved by aggregating results in a common accumulator after `itersBeforeAccum` iterations, balancing between driver communication and redundant computations.
</div><br>

---
<table><tr><td>2.<ins>HelperFunctions.scala</ins></td></tr></table>
<div style="text-align: justify;">
This file houses essential helper functions for deserialization, implementing the SimRank and random walk algorithms, and generating YAML outputs crucial to the application.
</div><br>

---
<table><tr><td>3.<ins>MapReduceProgram.scala</ins></td></tr></table>
<div style="text-align: justify;">
This file implements two Map-Reduce jobs and generates a predicted YAML file. Ensure Hadoop is configured locally before running. For instance, on Windows, start your local Hadoop cluster using `start-all.cmd`. Execute this file to produce node and edge matching files from the Map-Reduce jobs and generate the predicted YAML file.
</div><br>

---
<table><tr><td>4.<ins>NetGraphAlgebraDefs.NetGraphComponent.scala</ins></td></tr></table>
<div style="text-align: justify;">
This file provides definitions for NetGameSim's `NodeObject`, `Action`, and related components.
</div><br>

---
<table><tr><td>5.<ins>NetGraph> NGStoText.scala</ins></td></tr></table>
<div style="text-align: justify;">
Contained here is code that interfaces with NetGameSim to generate text-based output files.
</div><br>

## Project Build-up Steps

#### 1. <ins>Implementation of Random Walk Algorithm</ins>

Graphs are generated from NetGameSim:
- **Original Graph:** Referenced as `originalNetGraph` in the code.
- **Perturbed Graph:** Referenced as `perturbedNetGraph` in the code.

The first step involves reading these graphs. To simulate communications in a network, random walks are performed over the perturbed graph. The Spark framework facilitates distributed processing. The random walk algorithm operates by traversing the graph from a starting node, attempting to explore paths not previously taken within the same partition.

#### 2. <ins>Implementation of Graph Comparison Algorithm</ins>

This project utilizes the SimRank algorithm to approximate graph comparisons. SimRank measures node similarity based on the co-occurrence of relationships with other nodes. For more information, refer to the [SimRank Wikipedia page](https://en.wikipedia.org/wiki/SimRank).

#### 3. <ins>Use of Spark Framework for Distributed Processing of Big Data Graphs</ins>

The Spark framework is employed to manage the implementation of random walks and SimRank computations. Key aspects include:
- Parallel execution of random walks across multiple partitions.
- Initiation of random walks from nodes with no incoming connections.
- Iterative random walk computations within each partition.
- Coordination between partitions to share results after specified iterations, minimizing redundant SimRank computations.

#### 4. <ins>MitM-Statistics Evaluation</ins>

Upon completion of random walks and SimRank computations, results are aggregated using a Spark Accumulator. In this final stage:
- Statistical analyses, such as successful and unsuccessful attack assessments, are conducted.
- Results are displayed on the command line and saved as a YAML file containing MitM attack statistics.


## Errors and their causes

The following errors may occur during execution:

1. **NodeStr: $strObj doesn't have 10 fields!"**
   - This error occurs when parsing a string object (`$strObj`) into a `NodeObject` that should contain 10 fields. If the parser fails to extract these fields, this error is thrown.

2. **"File does not exist at /path/"**
   - This error indicates that a specified graph file path in `application.conf` does not exist or is incorrect. Ensure that all file paths are correctly configured and accessible.

3. **NodeObject with id == 0 not found in the loaded graph nodes!**
   - This error occurs when attempting to find a `NodeObject` with an `id` of `0` in the loaded graph nodes, but such a node does not exist. Verify that the initial node with `id == 0` is correctly loaded in the graph data.

4. **"Couldn't load perturbed graph File at _**
   - This error message signifies that the application encountered an issue while attempting to load the perturbed graph file specified in the configuration (`application.conf`). Check that the file path and name are correct and that the file is accessible.

5. **masterURL should be set to either on local path, hdfs localhost path, or s3 bucket path**
   - This error indicates that the `masterURL` parameter in `application.conf` is not set correctly. It should be configured to either a local file system path (`local`), a Hadoop Distributed File System (HDFS) localhost path (`hdfs://localhost:port`), or an Amazon S3 bucket path (`s3://your-bucket-name`). Ensure that `masterURL` is set to one of these valid options.

6. **Unsupported masterURL parameter.**
   - This error is thrown when the `masterURL` parameter in `application.conf` is set to an unsupported or invalid value. Verify and correct the `masterURL` configuration to resolve this issue.


## Scope for Improvement

1. **Optimize the SimRank Algorithm:**
   - Explore techniques such as algorithmic refinements and optimization strategies to enhance the efficiency and performance of the SimRank algorithm.

2. **Enhance Random Walk Algorithm and Initial Node Detection:**
   - Improve the random walk algorithm by considering advanced methods or heuristics that can optimize traversal and node selection processes.
   - Refine initial node detection mechanisms to ensure efficient identification and utilization of starting nodes in large-scale graphs.

3. **Incorporate Parallelization for Increased Efficiency:**
   - Implement parallel computing techniques, possibly leveraging frameworks like Apache Spark, to parallelize computations and operations within the application.
   - Distribute computational tasks across multiple cores or nodes to improve overall throughput and reduce execution times.


## YouTube link: <ins>[Project Demonstration + spark-submit](https://youtu.be/2-S-70iQDiM)</ins>

## Need Help?

Reach out to me!
**Email:** [punit.malpani@gmail.com](mailto:punit.malpani@gmail.com)
