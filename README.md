# MitM--Statistics-Simulator
Man-in-the-Middle Attack Statistics Simulator using Spark/Scala

#### Name: Punit Malpani
#### email: pmalpa2@uic.edu

# Project Title: MitM-Satistics Simulator (using Apache Spark)
# Project Description: 
This project aims to develop a distributed program for simulating and computing statistics for a Man-in-the-Middle attack by efficiently processing large graphs (resembling a real-world, large company's internal network) generated by the NetGraphSim platform. The primary objective is to compute successful and unsuccessful MitM attack statistics based on random communications within the network. This project is built on top of NetGraphSimulator project developed by Prof. Mark Grechanik [NetGameSim](https://github.com/0x1DOCD00D/NetGameSim).


## Tasks:
### 1. Implemenatiton of Random Walk algorithm
A pair of graph is generated from NetGameSim- 1)Original Graph (referenced as originalNetGraph in code) 2)Perturbed Graph(referenced as perturbedNetGraph in code). 
As first step of this project, graphs are read in. In order to simulate communications in a network, random walks are performed over the perturbed graph. Spark framework is used to facilitate distributed processing. Random walk algorithm takes in an entire graph, starting node, and visited nodes from the previous iteration in the same partiton, and perform a random walk, trying to take a different route from previously taken ones.

### 2. Implementation of Graph comparison algorithm
This project uses the famous SimRank algorithm to deal give an approximation solution the graph comparison. The key idea behind SimRank is that nodes are similar if they are related to similar nodes.The algorithm quantifies similarity based on the co-occurrence of relationships between pairs of nodes. Link: [https://en.wikipedia.org/wiki/SimRank]. 

### 3. Use of Spark framework to distributed processing of big data graphs.
As the third step, spark framework is used to facilitate the implementation of the random walks and SimRank. Random walks are performed on several partitions, starting with a node with no incoming connections to it. Random walks run parallely on various partitions with a potentially different starting point. Within the partitions, several iterations of random walk are run, with output of one iteration being used by the next to reduce simrank comparisons. Output of these partitions are communicated wit each other after a certain number of iterations complete, in order to reduce redundant simrank computations by all partitions.

### 4.MitM- Statistics Evaluation:
After random walks and simrank computations terminate, the result is accumulated in a Spark Accumulator. In this final stage, all the parallel random walks and simranks are analyzed and statistical results like successful/unsuccessful attacks/walks are produced. These are printed on the command line, as well as saved as another yaml file containing Mitm-statistics.

# How to install and run the project
Requirements: [Java Development Toolkit (JDK)](https://www.oracle.com/java/technologies/javase/jdk11-archive-downloads.html) and [Simple Build Toolkit (SBT)](https://www.scala-sbt.org/1.x/docs/index.html)


(This project uses case classes from [NetGameSim](https://github.com/0x1DOCD00D/NetGameSim) developed by Prof. Mark Grechanik. These case classes define the nodes and edges of the graphs. Assuming NetGraphSim is working on the system, this project takes 2 graphs files as input - 1) a .txt file, 2) .txt.perturbed file. **Note:NetGameSim produces ".ngs" and ".ngs.perturbed" files. Additional code (present in this repository's path: *base directory/NetGraph/src/main/scala/NGStoText.scala* should be added in the NetGameSim's *base directory/src/main/scala/* directory as per instruction in the comments of NGStoText.scala, in order to produce ".txt", and ".txt.perturbed" files that are text file representations of the same graphs.**

## Installations Used
+ Install Simple Build Toolkit (SBT)[MSI Installer](https://www.scala-sbt.org/1.x/docs/Installing-sbt-on-Windows.html)
+ Ensure you can create, compile and run Java and Scala programs.
+ Development Environment
+ Windows 11
+ Oracle OpenJDK 11.0.2
+ Scala 2.12.15 (compatible with AWS EMR- 6.13.0)
+ sbt version - 1.8.3 (in project>build.properties)
+ Spark 3.4.1 (compatible with AWS EMR- 6.13.0)
+ Other dependencies exist in build.sbt
+ IntelliJ IDEA Ultimate


# How to use the project
a). Clone the project from gihub. The base directory should be "CS441- MitMStatSim"<br>
b). [Check Step]: Check for NetGraphComponent.scala file in /NetGraph/src/main/scala/NetGraphAlgebraDefs/ directory. If its missing, copy this file from netmodelsim and put it in the exactly mentioned order. Make sure NetGraphAlgebraDefs is a package. 

1. As first step, go to the application.conf file and set the configuration. This file contains all folder and file path settings required for this project.
All the configurations are set in the Utilities module >src>main>resources>application.conf file. 

- **NGSGraphDir** - path to the directory where all the graph files (someOriginalGraph.txt, somePerturbedGraph.txt.perturbed) are stored <br>
- **originalGraphFileName**- file name of the original graph (required .txt extension) <br>
- **perturbedGraphFileName**- file name of the original graph (required .txt.perturbed extension) <br>
- **statisticsOutputFileName**- file name of the final statistics output file (required .yaml extension) br<> 
- **masterURL** = This decides the file system. This should be set either = "hdfs://localhost:*port*" or "local" or "s3://<*your_bucket_name*>". (This will decide and invoke different functions inside the application, so set this carefully." <br>
- **master** = This decides what Spark master type should be set. [Spark allowed master-url section](https://spark.apache.org/docs/latest/submitting-applications.html). It can be commented off if master is being passed inside spark-submit with --master *master-type*<br>
- **randomWalkCoeff** = decides what fraction of nodes to traverse in 1 random walk on a graph<br>
- **numOfParallelWalks** = decides number of partitions- each assigned a starting node<br>
- **numItersPerCompNode** = decides total random walks to be run per partition<br>
- **itersBeforeAccum** = decides number of iterations to run in every partition before accumulating results and letting other partitions know<br>
- **nodeMatchThreshold** = #simRank threshold- node matching below this threshold are rejected<br>


**Note Same variables exist for local, hdfs, or aws' s3 environment. One command line argument needs to be passed at run time for your environment (mentioned under point 4 below).**

2. Create folders as set in the application.conf. For easiness, use the input and output folders present in the root directory. Configure the application.conf such that all the input graph (text formatted) files (from NetGameSim) are in the input folder, and output statistics yaml file is saved in the output folder.

3. Save the 2 graph files with .txt, .txt.perturbed in the *NGSGraphDir* folder.

4. From the terminal, run *sbt compile*. Then, run "**sbt "run args(0)"**. One command line argument is required to run. The *args(0)* can be set either = "**local**" or "**hdfs**" or "**aws**". It decides the environment variables that should be loaded from application.conf.

## Working Explanation
(Italics are used to reference application.conf parameters)
### 1. Main.scala
This is the Main scala file which contains the main() function. It basically calls helper functions to first read the graph files and set spark configuration. It parses the graph to see potential starting nodes that do not have any incoming edges. Number of partitions for the spark application are decided by *numOfParallelWalks*. Each partition is assigned a starting node. Both the original and perturbed graphs are broadcasted and saved to all the computing units. After this, parallel random walks are run on different computing units. *numItersPerCompNode*- these many iterations are run per computing nodes, and results are send to a common accumulator after *itersBeforeAccum* iterations. All the partitions can read the accumulator data. This is how opimization is performed: neither results are send too frequently to the driver, nor too many redundant simrank computations are performed. 

### 2. HelperFunctions.scala
This file contains helper functions of all sort including deserialization, simRank algorithm, random walk algorithm, creating yaml output etc. which are integral to this application. 

### 3. MapReduceProgram.scala
This file contains the 2 Map-Reduce Jobs. It also calculates and generates a predicted yaml file. 
Assuming that hadoop is configured in the system, Start your local hadoop cluster using. e.g., for windows, use start-all.cmd 
<b>Second, run this file to generate node and edge matching file (as output of 2 map reduce jobs) and a predicted yaml file.</b> 

### 3. NetGraphAlgebraDefs.NetGraphComponent.scala
This file enables provides NetGameSim's **NodeObject** and **Action**, and other related definitions.

### 3. NetGraph> NGStoText.scala
This file contains code that can be used with NetGameSim to generate text output files.


## FOR running in hadoop locally using general file system

Steps to run:
1. Set paths under local scope in application.conf (see description at the top) - remember - all paths should conform to the local FS. 
2. Ensure that spark-core depency is present in build.sbt and not marked as % "provided".
3. from the terminal go to the Main.scala path and run *sbt compile* then *sbt "run local"* (with inverted commas). This should run the program and create 1 yaml file with MitM- simulation statistics as per the parameters set in the application.conf.

## FOR running on Amazon EMR
Steps to run:
1. Set paths under aws scope in application.conf set masterURL = "s3://<your-bucket-name>, in application.conf. Set other paths (see description at the top). remember - all paths should conform to the amazon s3 file system and start with "s3".
2. From the terminal, go to the Main.scala path and run *"sbt clean compile assembly"*. This should create fat jar file named MitmStatSim.jar in *basedirectory*/target/scala-2.12/ 
3. create 2 directories in the aws bucket- one for Graphs text file inputs, and second for output yaml. 
4. Upload the graph files to the *input* in s3 bucket. Upload the jar file to the s3 root directory. (No specific need to create these exact folders, this is just one way to configure)
5. Now create a cluster in the amazon emr. Use spark application in step. Provide the custom jar s3 file path url.  In the spark-submit section, use the following:<br>
*--class mitm.Main<br>
--jars /your/s3/jar/file/path<br>
--driver-class-path /your/s3/jar/file/path<br>*

6. Now go Set command line argument = aws.

# ERRORS 
(lossely handled)
Following Error will be thrown for corresponding situations:
1. **NodeStr: $strObj doesn't have 10 fields!"**- If 10 fields couldn't be parsed out of a string object corresponding to a NodeObject that should have 10 fields.
    
2. **"File does not exist at /path/"** - If a graph file path referenced in application.conf is erroneous.
  
3. **NodeObject with id == 0 not found in the loaded graph nodes!** - when loaded graph does not contain initial node with id = 0

4. **"Couldn't load perturbed graph File at _** - when original perturbed couldn't be loaded 
    
5. **masterURL should be set to either on local path, hdfs localhost path or s3 bucket path** - if masterURL parameter in application.conf is not set among these three
    
6.  **Unsupported masterURL parameter.** - when masterURL is improperly set and thus yaml cannot be created.

## Scope for improvement
1. SimRank algorithm can be definitely optimized.
2. Random Walk algorithm and initial starting node detections can be optimized. 
3. Parallelization can be incorporated to increase efficient.

### YouTube link:
[YouTube Link- project demonstration + spark-submit](https://youtu.be/2-S-70iQDiM)
